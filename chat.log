# ECG Self-Supervised Learning Project - Development Log

## Project Goal
Develop a self-supervised learning (SSL) model for ECG data that surpasses or matches ECG-JEPA's performance while maintaining a simpler architecture.

---

## Phase 1: Understanding ECG-JEPA (Baseline)

### What is ECG-JEPA?
ECG-JEPA is a masking-based SSL method for ECG signals. It uses:
- **Two encoders**: Context encoder + Target encoder (with EMA updates)
- **Masking strategy**: Block masking where 15-25% of patches are visible (context), 75-85% are masked (targets)
- **Predictor network**: Small transformer that predicts target patch embeddings from context
- **Loss**: L1/Smooth L1 between predicted and actual target embeddings
- **Collapse prevention**: EMA (Exponential Moving Average) on target encoder

### ECG-JEPA Results (Benchmark)
- **Rhythm classification AUC: 0.96** on PTB-XL
- This is our target to beat or match

---

## Phase 2: LeJEPA-ECG Implementation

### Why LeJEPA?
LeJEPA (from computer vision) uses a different paradigm:
- **Single encoder** (no EMA complexity)
- **Augmentation invariance**: Instead of masking, it creates multiple augmented views
- **SIGReg regularization**: Forces embeddings to follow isotropic Gaussian distribution to prevent collapse

### Hypothesis
We hypothesized that LeJEPA's simpler single-encoder architecture with SIGReg could work for ECG data, avoiding the complexity of EMA.

### Implementation Details

#### Files Created
- `LeJEPA-ECG/models/lejepa.py` - Main model with SIGReg
- `LeJEPA-ECG/models/vit.py` - Vision Transformer encoder
- `LeJEPA-ECG/models/modules.py` - Building blocks (Block, MLP, Projector)
- `LeJEPA-ECG/data/transforms.py` - ECG augmentations (scaling, noise, masking)
- `LeJEPA-ECG/pretrain.py` - Pretraining script with DDP and Wandb
- `LeJEPA-ECG/finetune.py` - Linear probing evaluation

#### Key Components
1. **SIGReg (Sketched Isotropic Gaussian Regularization)**
   - Compares empirical characteristic function of embeddings to standard Gaussian
   - Uses random projections (num_slices=1024) for efficiency
   - Forces diversity in learned representations

2. **Augmentations for ECG**
   - Random amplitude scaling
   - Gaussian noise injection
   - Random channel dropout
   - Temporal masking

3. **Training Setup**
   - Multi-GPU support via DDP (DistributedDataParallel)
   - Wandb logging with embedding visualizations
   - Configurable via YAML files

### Infrastructure Improvements Made
1. **Reproducibility**: Added `--seed` argument and `set_seed()` function
2. **DDP Support**: Proper distributed training with gradient sync
3. **Wandb Integration**: Team entity support (`--entity AtlasVision_CC`)
4. **Throughput Calculation**: Fixed to account for gradient accumulation and world size
5. **Data Loading**: Optimized with `num_workers`, `prefetch_factor`, `persistent_workers`

### LeJEPA-ECG Results

#### Pretraining
- Trained for 100k steps on MIMIC-IV-ECG
- Used 2x RTX 6000 Ada GPUs (batch_size=440, acc=4)
- Also trained on single A100 80GB (batch_size=768, acc=2)

#### Finetuning Results
- **Rhythm classification AUC: ~0.59** (random baseline ~0.5)
- This is significantly worse than ECG-JEPA's 0.96

### Why LeJEPA Failed for ECG

**Root Cause Analysis:**
1. **Augmentation-invariance doesn't fit ECG**: LeJEPA forces the model to produce similar embeddings for different augmented views. But for ECG:
   - Temporal structure is critical (arrhythmias are defined by timing)
   - Augmentations that preserve class labels might destroy temporal patterns
   - The model learns to ignore temporal information to be "invariant"

2. **No temporal prediction task**: Unlike masking-based methods, LeJEPA doesn't require the model to understand temporal relationships - it just needs views to match.

3. **SIGReg alone isn't enough**: While SIGReg prevents complete collapse, the representations learned are not semantically meaningful for ECG.

---

## Phase 3: MyJEPA - Hybrid Approach

### Design Philosophy
Combine the best of both worlds:
- **From ECG-JEPA**: Masking + prediction task (forces temporal understanding)
- **From LeJEPA**: Single encoder + SIGReg (simpler than EMA)

### MyJEPA Architecture

```
Input ECG (B, 12, 5000)
       │
       ▼
   MaskCollator
       │
       ├──► Context indices (15-25% visible)
       │
       ▼
   Single Encoder (shared weights)
       │
       ├──► encode(x, context_mask) → z_ctx
       │
       └──► encode(x, full) → z_full
                   │
                   ├──► extract targets → z_tgt
                   │
                   └──► mean pool → z_global
                              │
                              ▼
                         Projector → z_proj
                              │
                              ▼
                           SIGReg
       
   Predictor(z_ctx) → z_pred
       
   Loss = pred_loss(z_pred, z_tgt) + λ * sigreg_loss(z_proj)
```

### Key Differences from ECG-JEPA

| Aspect | ECG-JEPA | MyJEPA |
|--------|----------|--------|
| Encoders | 2 (context + target with EMA) | 1 (single encoder) |
| Collapse prevention | EMA momentum | SIGReg regularization |
| Target gradient | stop_gradient | Normal gradient flow |
| Complexity | Higher (EMA params) | Lower |

### Implementation Files

#### Core Model
- `MyJEPA/models/myjepa.py` - Main model class with:
  - VisionTransformer encoder
  - Predictor network (from ECG-JEPA)
  - Projector + SIGReg (from LeJEPA)

#### Copied/Adapted
- `MyJEPA/models/vit.py` - From LeJEPA-ECG
- `MyJEPA/models/predictor.py` - From ECG-JEPA
- `MyJEPA/models/modules.py` - Combined from both
- `MyJEPA/data/masks.py` - MaskCollator from ECG-JEPA

### Initial MyJEPA Results

#### Pretraining Observations
- Trained for ~50k+ steps
- `embeddings/std` dropped from ~1.0 to ~0.7 during training
- PCA plots showed some clustering but not as clear as expected

#### Finetuning Results
- **Rhythm classification AUC: 0.59** (same as LeJEPA!)
- This indicated the model was still experiencing some form of collapse

### Diagnosis: Why 0.59 AUC?

**The Problem:**
Without asymmetry (stop-gradient or EMA), the model can minimize prediction loss by:
1. Mapping all embeddings to similar values (collapse)
2. The prediction becomes trivial: "everything predicts to the same thing"

**SIGReg's Limitation:**
- SIGReg is applied to projected embeddings (after the Projector network)
- The Projector can learn to "spread" collapsed encoder outputs artificially
- The raw encoder embeddings may still collapse!

---

## Phase 4: Fixing MyJEPA - Enhanced Regularization

### Latest Changes (Current Session)

#### Problem Identified
SIGReg on projected embeddings can be "fooled" by the projector - it can take collapsed encoder outputs and spread them to look Gaussian.

#### Solution Implemented
Three-pronged regularization approach:

1. **SIGReg on Raw Encoder Outputs** (`sigreg_raw_lambda=0.1`)
   - Applied directly to `z_global` (mean-pooled encoder output)
   - Cannot be fooled by the projector
   - Forces encoder itself to produce diverse embeddings

2. **SIGReg on Projected Embeddings** (`sigreg_lambda=0.02`)
   - Keep original regularization as secondary signal

3. **Explicit Variance Constraint** (`variance_lambda=0.1`, `variance_target=1.0`)
   - Penalizes when std(embeddings) < 1.0
   - Directly prevents embedding shrinking
   - Loss = max(0, target - std)

#### New Loss Function
```
total_loss = pred_loss + reg_loss

reg_loss = λ_proj * SIGReg(projected_embeddings)
         + λ_raw  * SIGReg(raw_encoder_embeddings)  
         + λ_var  * max(0, 1.0 - std(embeddings))
```

#### Files Modified
- `MyJEPA/models/myjepa.py` - Added `sigreg_raw`, variance constraint
- `MyJEPA/configs/ViTS_mimic.yaml` - Added new hyperparameters
- `MyJEPA/configs/ViTS_mimic_a100.yaml` - Added new hyperparameters
- `MyJEPA/pretrain.py` - Updated logging to show `reg_loss`

### Expected Outcome
- `embeddings/std` should stay ≥1.0 throughout training
- Better separation in embedding space
- Target: **AUC 0.92-0.95** on rhythm classification

---

## Technical Details

### Hardware Configurations Used

#### 2x RTX 6000 Ada Setup
```yaml
batch_size: 512
gradient_accumulation_steps: 2
# Effective batch: 512 × 2 GPUs × 2 acc = 2048
```

#### Single A100 80GB Setup
```yaml
batch_size: 768
gradient_accumulation_steps: 2
# Effective batch: 768 × 2 = 1536
```

### Wandb Integration
- Project: `MyJEPA-ECG` (and `LeJEPA-ECG`)
- Team: `AtlasVision_CC`
- Logged metrics: loss, pred_loss, reg_loss, lr, throughput, embeddings stats, PCA plots

### DDP Training
- Uses `torchrun` for launching
- Only main process (rank 0) logs to Wandb
- Gradients synchronized across GPUs

### Data Loading Optimization
- Initial loading is slow (full dataset into RAM)
- Considered preprocessing script but deemed unnecessary
- Used `num_workers=16`, `prefetch_factor=2` for faster iteration

---

## Key Learnings

1. **SSL paradigm matters for the domain**
   - Augmentation-invariance (contrastive, VICReg-style) doesn't work well for temporal data like ECG
   - Masking-prediction tasks force learning of temporal structure

2. **Asymmetry is crucial in prediction tasks**
   - Without stop-gradient or EMA, prediction tasks can collapse
   - Need explicit regularization to maintain embedding diversity

3. **Regularization placement matters**
   - Applying regularization after projector can be fooled
   - Must regularize the encoder outputs directly

4. **Embedding monitoring is essential**
   - Track `embeddings/std` during training
   - PCA/t-SNE visualizations help diagnose collapse early

---

## Next Steps

1. **Train with enhanced regularization** - Monitor if std stays ≥1.0
2. **Hyperparameter tuning** - Adjust λ values if needed
3. **Evaluate on PTB-XL** - Compare AUC with ECG-JEPA baseline
4. **Consider stop-gradient** - If regularization alone isn't enough, add SimSiam-style stop-gradient

---

## File Structure Reference

```
MY_ECG/
├── ECG-JEPA/          # Original ECG-JEPA (reference)
├── LeJEPA-ECG/        # Failed attempt with augmentation-invariance
│   ├── models/
│   ├── data/
│   ├── configs/
│   ├── batch/
│   ├── pretrain.py
│   └── finetune.py
├── MyJEPA/            # Current approach (masking + single encoder + SIGReg)
│   ├── models/
│   │   ├── myjepa.py      # Main model with enhanced regularization
│   │   ├── vit.py
│   │   ├── predictor.py
│   │   └── modules.py
│   ├── data/
│   │   ├── masks.py       # Block masking from ECG-JEPA
│   │   └── ...
│   ├── configs/
│   │   ├── ViTS_mimic.yaml
│   │   └── ViTS_mimic_a100.yaml
│   ├── batch/
│   ├── pretrain.py
│   └── finetune.py
└── dataset/           # MIMIC-IV-ECG, PTB-XL
```

---

## Results Summary Table

| Method | Architecture | AUC (Rhythm) | Notes |
|--------|-------------|--------------|-------|
| ECG-JEPA | 2 encoders + EMA | **0.96** | Baseline to beat |
| LeJEPA-ECG | 1 encoder + augmentation + SIGReg | 0.59 | Augmentation-invariance failed |
| MyJEPA v1 | 1 encoder + masking + SIGReg(proj) | 0.59 | Collapse despite SIGReg |
| MyJEPA v2 | 1 encoder + masking + SIGReg(raw) + variance | TBD | Enhanced regularization |

---

## Phase 5: NEPA-ECG Implementation

### Why NEPA?

After MyJEPA's struggles with collapse, we explored **NEPA (Next-Embedding Predictive Autoregression)** - a simpler SSL method from the NEPA paper. Key advantages:

- **No masking**: Unlike JEPA, no complex mask sampling needed
- **No predictor network**: Simpler architecture
- **No EMA**: Uses stop-gradient instead (like SimSiam)
- **Autoregressive prediction**: Each patch predicts the next patch's embedding

### NEPA Architecture

```
Input ECG (B, 12, 5000)
       │
       ▼
   PatchEmbedding (1D Conv)
   (B, 12, 5000) → (B, 200, 384)
       │
       ▼
   + Positional Embeddings (sinusoidal)
       │
       ▼
   [Causal Transformer × 8]
   ├─► Causal Attention (patch t sees only 0...t-1)
   └─► MLP (384 → 1536 → 384)
       │
       ├─► z_in: Input embeddings (after patch+pos embed)
       └─► z_out: Output embeddings (after transformer)
       
   Loss = -cosine_similarity(z_out[:-1], stop_grad(z_in[1:]))
```

### Key NEPA Concepts

1. **Causal Attention**: Each patch can only attend to previous patches (upper triangular mask)
2. **Stop-Gradient on Targets**: Prevents collapse by breaking gradient symmetry
3. **Shifting for Prediction**: Output at position t predicts input at position t+1
4. **Negative Cosine Similarity**: Loss approaches -1.0 when predictions are perfect

### Implementation Details

#### Files Created (nepa-ecg/)
- `models/nepa_ecg.py` - Main NEPAForECG model
- `models/modules.py` - PatchEmbedding, CausalAttention, CausalBlock, MLP, LayerScale, DropPath
- `models/vit_classifier.py` - Classifier for finetuning
- `data/datasets.py` - ECG dataset classes
- `data/transforms.py` - TrainTransformECG, EvalTransformECG
- `pretrain.py` - Pretraining with DDP and Wandb
- `finetune.py` - PTB-XL evaluation
- `configs/ViTS_mimic.yaml`, `ViTB_mimic.yaml`, `ViTL_mimic.yaml`
- `batch/pretrain/small.sh`, `base.sh`, `large.sh`
- `batch/finetune/rhythm.sh`, etc.

#### Model Configurations

| Config | dim | depth | heads | params |
|--------|-----|-------|-------|--------|
| ViT-S  | 384 | 8     | 6     | ~22M   |
| ViT-B  | 768 | 12    | 12    | ~86M   |
| ViT-L  | 1024| 24    | 16    | ~307M  |

### Pretraining Results

#### Training Dynamics
- **Loss curve**: Started at ~0, went negative (good!), stabilized around -0.3 to -0.4
- **Embeddings/std**: Stabilized around 0.14-0.15 (lower than expected)
- **Embeddings/norm**: ~5.5
- Trained for 200k steps on MIMIC-IV-ECG

#### Loss Interpretation
- NEPA uses negative cosine similarity
- Loss of -0.3 means ~60% alignment between predictions and targets
- More negative = better (perfect would be -1.0)

### Finetuning Results

#### Initial Results
- **Rhythm classification AUC: 0.84** (vs ECG-JEPA's 0.96)
- Significant gap of ~12% AUC

#### Data Pipeline Issues Identified

Compared our finetuning with ECG-JEPA's and found critical differences:

1. **Normalization**: We used hardcoded mean/std, ECG-JEPA computes from train set
2. **Clipping**: ECG-JEPA clips signals to [-5, 5], we didn't
3. **Cropping**: ECG-JEPA uses random crops (train) and strided crops with averaging (eval)

#### Fixes Applied

```python
# 1. Compute normalization from train data
mean = np.mean(x[train_mask], axis=(0, 1), keepdims=True)
std = np.std(x[train_mask], axis=(0, 1), keepdims=True)
x = (x - mean) / (std + 1e-6)

# 2. Clip to [-5, 5]
x = np.clip(x, -5, 5)

# 3. Random crop for training
class TrainTransformECG:
    def __call__(self, x):
        start = random.randint(0, x.shape[-1] - crop_size)
        return x[..., start:start + crop_size]

# 4. Strided crops with averaging for eval
class EvalTransformECG:
    def __call__(self, x):
        crops = []
        for start in range(0, x.shape[-1] - crop_size + 1, stride):
            crops.append(x[..., start:start + crop_size])
        return torch.stack(crops)  # Average logits during inference
```

#### Results After Fixes
- **Rhythm classification AUC: 0.88** (+4% improvement)
- Still ~8% gap from ECG-JEPA's 0.96

### Architectural Gaps Identified

After detailed comparison with original NEPA and ECG-JEPA:

| Component | Original NEPA | Our NEPA-ECG | ECG-JEPA |
|-----------|---------------|--------------|----------|
| CLS/Register Token | Yes (CLS) | No | Yes (registers) |
| Position Embedding | RoPE | Sinusoidal | Sinusoidal |
| QK Normalization | Yes | Yes | No |
| LayerScale | Yes | Yes | No |
| Bias in Linear | qkv_bias=True | True | False |
| Warmup Steps | 10k | 5k | 10k |
| Optimizer Betas | [0.9, 0.99] | [0.9, 0.999] | [0.9, 0.99] |

### Why These Gaps Matter

1. **CLS/Register Tokens**: Provide a global aggregation point; without them, we rely on mean pooling which may lose information

2. **RoPE vs Sinusoidal**: RoPE encodes relative positions directly in attention, may generalize better to variable lengths

3. **Warmup Steps**: Longer warmup (10k vs 5k) allows more stable initial training

4. **Bias Settings**: Can affect optimization dynamics

### Technical Issues Encountered

1. **BFloat16 Error**: `TypeError: Got unsupported ScalarType BFloat16`
   - Fix: Convert to float32 before numpy: `tensor.cpu().float().numpy()`

2. **Pickle Error with weights_only=True**: PyTorch 2.6+ requires explicit `weights_only=False` for checkpoints with numpy scalars
   - Fix: `torch.load(path, weights_only=False)`

---

## Phase 6: Plan - Rebuild on Original NEPA Codebase

### Rationale
Instead of building from scratch, adapt the original NEPA codebase (`nepa/`) to ECG. This ensures all NEPA components are preserved:
- CLS token
- RoPE (Rotary Position Embedding)
- QK normalization
- LayerScale
- Proper initialization
- HuggingFace Trainer integration

### Planned Changes (Minimal)

1. **Create `nepa/models/vit_nepa_ecg/`**
   - `configuration_vit_nepa_ecg.py`: ECG-specific config (signal_length, num_channels=12)
   - `modeling_vit_nepa_ecg.py`: 
     - Conv1d patch embedding (instead of Conv2d)
     - 1D RoPE (instead of 2D grid)

2. **Create `nepa/run_nepa_ecg.py`**
   - Load MIMIC-IV-ECG .npy files
   - ECG transforms (normalize, clip, crop)
   - Use HuggingFace Trainer (from original NEPA)

3. **Create `nepa/run_ecg_classification.py`**
   - PTB-XL finetuning
   - Bidirectional attention for classification

4. **Bash scripts**
   - `nepa/scripts/pretrain/nepa_ecg_small.sh`
   - `nepa/scripts/pretrain/nepa_ecg_base.sh`

### Expected Outcome
- All NEPA components preserved
- Should close the gap to ECG-JEPA's 0.96 AUC
- Cleaner codebase (leverages HuggingFace infrastructure)

---

## Updated Results Summary Table

| Method | Architecture | AUC (Rhythm) | Notes |
|--------|-------------|--------------|-------|
| ECG-JEPA | 2 encoders + EMA | **0.96** | Baseline to beat |
| LeJEPA-ECG | 1 encoder + augmentation + SIGReg | 0.59 | Augmentation-invariance failed |
| MyJEPA v1 | 1 encoder + masking + SIGReg(proj) | 0.59 | Collapse despite SIGReg |
| MyJEPA v2 | 1 encoder + masking + SIGReg(raw) + variance | TBD | Enhanced regularization |
| NEPA-ECG v1 | Causal attention + stop-grad | 0.84 | Initial implementation |
| NEPA-ECG v2 | + Fixed data pipeline | 0.88 | After normalization/cropping fixes |
| NEPA-ECG v3 | Rebuild on original NEPA | TBD | Planned: add CLS, RoPE, etc. |

---

## Key Learnings from NEPA-ECG

1. **Data pipeline is critical**: Proper normalization, clipping, and cropping account for ~4% AUC improvement

2. **Architectural details matter**: Small differences (CLS token, RoPE, warmup) can account for remaining performance gap

3. **Build on reference implementations**: Instead of reimplementing from scratch, adapt existing codebases to preserve all components

4. **NEPA is simpler than JEPA**: No masking, no predictor, no EMA - just causal attention with stop-gradient

5. **Negative cosine similarity loss**: More negative = better; -0.3 to -0.4 is reasonable, -1.0 would be perfect

---

*Log created: December 17, 2025*
*Last updated: December 25, 2025*

