# LeJEPA-ECG Configuration (Default - RTX 6000 Ada / 48GB VRAM)
# ViT-Small encoder for MIMIC-IV ECG pretraining
#
# GPU-specific configs available:
#   - ViTS_mimic_rtx6000.yaml  → RTX 6000 Ada (48GB): batch=440, grad_accum=4, ~42GB
#   - ViTS_mimic_a100.yaml     → A100 (80GB):        batch=700, grad_accum=2, ~67GB (~1.6× faster)

# Data
sampling_frequency: 500
channels:
  - I
  - II
  - III
  - AVR
  - AVL
  - AVF
  - V1
  - V2
  - V3
  - V4
  - V5
  - V6
channel_size: 5000
patch_size: 25

# Datasets
datasets:
  mimic-iv-ecg: 1.0

# Model Architecture
dim: 384
depth: 8
num_heads: 6
mlp_ratio: 4.0
qkv_bias: false
dropout: 0.0
attn_dropout: 0.0
num_registers: 1
bias: true
norm_eps: 1.0e-6
layer_scale_eps: 0.0

# Projector
proj_hidden_dim: 2048
proj_dim: 128

# LeJEPA
num_views: 4
lambda: 0.02
num_slices: 1024

# Augmentation
crop_scale:
  - 0.5
  - 1.0
amplitude_scale:
  - 0.8
  - 1.2
noise_std: 0.05

# Training
steps: 100000
batch_size: 440  # 440 × 4 views = 1760 samples → ~42GB VRAM (RTX 6000 tested)
learning_rate: 0.001
final_learning_rate: 1.0e-6
learning_rate_warmup_steps: 5000
weight_decay: 0.05
opt_betas:
  - 0.9
  - 0.999
opt_eps: 1.0e-8
gradient_clip: 1.0
gradient_accumulation_steps: 4  # Effective batch = 440 × 4 = 1760

# Checkpointing
checkpoint_interval: 5000

# Wandb
wandb_project: "LeJEPA-ECG"
wandb_log_interval: 10
wandb_viz_interval: 1000

