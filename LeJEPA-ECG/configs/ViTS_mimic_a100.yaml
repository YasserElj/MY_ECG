# LeJEPA-ECG Configuration - A100 (80GB VRAM)
# batch_size=512 × 4 views = 2048 samples/forward → ~50GB peak VRAM
# 2× faster than RTX 6000 config with same effective batch size

# Data
sampling_frequency: 500
channels: [I, II, III, AVR, AVL, AVF, V1, V2, V3, V4, V5, V6]
channel_size: 5000
patch_size: 25

# Datasets
datasets:
  mimic-iv-ecg: 1.0

# Model Architecture (ViT-Small)
dim: 384
depth: 8
num_heads: 6
mlp_ratio: 4.0
qkv_bias: false
dropout: 0.0
attn_dropout: 0.0
num_registers: 1
bias: true
norm_eps: 1.0e-6
layer_scale_eps: 0.0

# Projector
proj_hidden_dim: 2048
proj_dim: 128

# LeJEPA
num_views: 4
lambda: 0.02
num_slices: 1024

# Augmentation
crop_scale: [0.5, 1.0]
amplitude_scale: [0.8, 1.2]
noise_std: 0.05

# Training - A100 (80GB) - 2× faster than RTX 6000 config
steps: 100000
batch_size: 512              # ~50GB peak VRAM
gradient_accumulation_steps: 2   # Effective batch = 1024
learning_rate: 0.001
final_learning_rate: 1.0e-6
learning_rate_warmup_steps: 5000
weight_decay: 0.05
opt_betas: [0.9, 0.999]
opt_eps: 1.0e-8
gradient_clip: 1.0

# Checkpointing
checkpoint_interval: 5000

# Wandb
wandb_project: "LeJEPA-ECG"
wandb_log_interval: 10
wandb_viz_interval: 1000

