# ViT-Small config for I-JEPA pretraining on MIMIC-IV-ECG
# learnable encoder parameters: 14,277,888

# data
sampling_frequency: 500
channels: [I, II, III, AVR, AVL, AVF, V1, V2, V3, V4, V5, V6]
channel_size: 5000
patch_size: 25

# --- I-JEPA Masking Strategy ---
masking_strategy: 'i-jepa'
context_scale: [0.85, 0.95]  # Context is ONE contiguous block covering 85-95%
pred_scale: [0.15, 0.20]     # Each target block covers 15-20% of the signal
n_pred_blocks: 4             # Number of target blocks to predict
min_keep: 10                 # Minimum context patches to keep

datasets:
  mimic-iv-ecg: 1.

# model architecture
dim: 384
depth: 8
num_heads: 6
pred_dim: 192
pred_depth: 8
pred_num_heads: 6
mlp_ratio: 4.
qkv_bias: False
dropout: 0.
attn_dropout: 0.
num_registers: 1
bias: False
norm_eps: 1.0e-6
layer_scale_eps: 0.

# training
steps: 100_000
batch_size: 2048
encoder_momentum: 0.998
final_encoder_momentum: 0.9995
learning_rate: 1.0e-3
final_learning_rate: 1.0e-6
learning_rate_warmup_steps: 10_000
weight_decay: 1.0e-2
final_weight_decay: 1.0e-1
opt_betas: [0.9, 0.99]
opt_eps: 1.0e-6
gradient_clip: 0
gradient_accumulation_steps: 1
checkpoint_interval: 10_000

