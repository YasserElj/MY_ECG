# MyJEPA ViT-Small config for A100 80GB GPU
# Larger batch size for faster training

# Data
sampling_frequency: 500
channels: [I, II, III, AVR, AVL, AVF, V1, V2, V3, V4, V5, V6]
channel_size: 5000
patch_size: 25

# Masking (from ECG-JEPA)
min_block_size: 10
min_keep_ratio: 0.15
max_keep_ratio: 0.25

# Datasets
datasets:
  mimic-iv-ecg: 1.

# Encoder architecture (ViT-Small)
dim: 384
depth: 8
num_heads: 6
mlp_ratio: 4.
qkv_bias: false
dropout: 0.
attn_dropout: 0.
num_registers: 1
bias: false
norm_eps: 1.0e-6
layer_scale_eps: 0.

# Predictor architecture
pred_dim: 192
pred_depth: 8
pred_num_heads: 6

# SIGReg parameters (from LeJEPA)
proj_hidden_dim: 2048
proj_dim: 128
num_slices: 1024
sigreg_lambda: 0.02          # SIGReg on projected embeddings (original)
sigreg_raw_lambda: 0.1       # SIGReg on raw encoder outputs (NEW - can't be fooled)
variance_lambda: 0.1         # Explicit variance constraint (NEW - prevents shrinking)
variance_target: 1.0         # Target std for variance constraint

# Training (optimized for A100)
steps: 100_000
batch_size: 512
gradient_accumulation_steps: 2
learning_rate: 1.0e-3
final_learning_rate: 1.0e-6
learning_rate_warmup_steps: 5_000
weight_decay: 1.0e-2
gradient_clip: 1.0
checkpoint_interval: 5_000

# DataLoader
num_workers: 8  # Adjust based on CPU cores

# Wandb
wandb_project: 'MyJEPA-ECG'
wandb_log_interval: 10
wandb_viz_interval: 1000

